{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.lbGraficas import cargaDataSet\n",
    "from lib.lbGraficas import showGraficaCont\n",
    "from lib.lbClima import cargaDfClima\n",
    "from lib.lbClima import quitarHora\n",
    "from lib.lbVarios import series_to_supervised\n",
    "from lib.lbCargaDatosNorm import cargaDatosSep\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from os import path\n",
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ya no hace falta esta carga\"\"\"\n",
    "#dfSol = cargaDatosSep('datos/datosContaminacion/datos17.csv',';')\n",
    "#print (\"=======\")\n",
    "#print (dfSol)\n",
    "#print (dfSol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "hInicio = time.strftime(\"%H-%M-%S\")\n",
    "#print (time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "# load the network weights\n",
    "#filepath=\"checkpoint/weightsV1.hdf5\"\n",
    "#model.load_weights(filepath) #Descomentar cuando tengamos un checkpoint\n",
    "#fecha_archivos = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#fecha_archivos = time.strftime(\"%Y-%m-%d_%H)\n",
    "#print(fecha_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -------------------- load dataset -------------------- \"\"\"\n",
    "dataset14 = cargaDataSet(\"datos/datosContaminacion/datos2014_DatosDiarios.txt\") #datos 2014\n",
    "dataset15 = cargaDataSet(\"datos/datosContaminacion/datos2015_DatosDiarios.txt\") #datos 2015\n",
    "dataset16 = cargaDataSet(\"datos/datosContaminacion/datos2016_DatosDiarios.txt\") #datos 2016\n",
    "#dataset = pandas.concat([dataset15,dataset16])\n",
    "#dataset = pandas.concat([dataset14,dataset15,dataset16])\n",
    "#print(dataset.head())\n",
    "print(dataset15.shape)\n",
    "print(dataset16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para 2017 y 2018 cambia el formato del txt, entonces lo cogemos en csv ya directamente\n",
    "dataset17 = pandas.read_csv('datos/datosContaminacion/datos17.csv',sep=';')\n",
    "df17 = dataset17.drop(['V01', 'V02', 'V03', 'V04', 'V05', 'V06', 'V07', 'V08', 'V09', 'V10','V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20','V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31'], axis=1)\n",
    "df17['Tecnica']='00'\n",
    "df17['Periodo_Analisis']='04'\n",
    "df17Filtrado = df17.loc[df17['ESTACION'] == 4]\n",
    "df17Filtrado['ESTACION']='28079004'\n",
    "dataset17 = df17Filtrado[['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']]\n",
    "#dataset17 = df17Filtrado.loc['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']\n",
    "dataset17.columns = ['Estacion','Magnitud','Tecnica','Periodo_Analisis','Año','Mes',\"Dia1\",\"Dia2\",\"Dia3\",\"Dia4\",\"Dia5\",\"Dia6\",\"Dia7\",\"Dia8\",\"Dia9\",\"Dia10\",\"Dia11\",\"Dia12\",\"Dia13\",\"Dia14\",\"Dia15\",\"Dia16\",\"Dia17\",\"Dia18\",\"Dia19\",\"Dia20\",\"Dia21\",\"Dia22\",\"Dia23\",\"Dia24\",\"Dia25\",\"Dia26\",\"Dia27\",\"Dia28\",\"Dia29\",\"Dia30\",\"Dia31\"]\n",
    "print(dataset17.shape)\n",
    "\n",
    "dataset18 = pandas.read_csv('datos/datosContaminacion/datos18.csv',sep=';')\n",
    "df18 = dataset18.drop(['V01', 'V02', 'V03', 'V04', 'V05', 'V06', 'V07', 'V08', 'V09', 'V10','V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20','V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31'], axis=1)\n",
    "df18['Tecnica']='00'\n",
    "df18['Periodo_Analisis']='04'\n",
    "df18Filtrado = df18.loc[df18['ESTACION'] == 4]\n",
    "df18Filtrado['ESTACION']='28079004'\n",
    "dataset18 = df18Filtrado[['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']]\n",
    "#dataset18 = df18Filtrado.loc['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']\n",
    "dataset18.columns = ['Estacion','Magnitud','Tecnica','Periodo_Analisis','Año','Mes',\"Dia1\",\"Dia2\",\"Dia3\",\"Dia4\",\"Dia5\",\"Dia6\",\"Dia7\",\"Dia8\",\"Dia9\",\"Dia10\",\"Dia11\",\"Dia12\",\"Dia13\",\"Dia14\",\"Dia15\",\"Dia16\",\"Dia17\",\"Dia18\",\"Dia19\",\"Dia20\",\"Dia21\",\"Dia22\",\"Dia23\",\"Dia24\",\"Dia25\",\"Dia26\",\"Dia27\",\"Dia28\",\"Dia29\",\"Dia30\",\"Dia31\"]\n",
    "print(dataset18.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.concat([dataset14,dataset15,dataset16,dataset17,dataset18])\n",
    "print(dataset.shape)\n",
    "plzEspana = \"28079004\"\n",
    "print(\"ahora filtramos y colocamos todos los días en orden\")\n",
    "values08 = showGraficaCont(dataset,plzEspana,\"08\",False) #valor de NO2 en cada día\n",
    "values8 = showGraficaCont(dataset,plzEspana,\"8\",True) #valor de NO2 en cada día\n",
    "values01 = showGraficaCont(dataset,plzEspana,\"01\",False) #valor de SO2 en cada día\n",
    "values1 = showGraficaCont(dataset,plzEspana,\"1\",True) #valor de SO2 en cada día\n",
    "values06 = showGraficaCont(dataset,plzEspana,\"06\",False) #valor de CO en cada día\n",
    "values6 = showGraficaCont(dataset,plzEspana,\"6\",True) #valor de CO en cada día\n",
    "print(\"Número de valores de 08: \" + str(len(values08)) + \" Y número de valores de 8: \" + str(len(values8)))\n",
    "print(\"Número de valores de 01: \" + str(len(values01)) + \" Y número de valores de 1: \" + str(len(values1)))\n",
    "print(\"Número de valores de 06: \" + str(len(values06)) + \" Y número de valores de 6: \" + str(len(values6)))\n",
    "values08 = values08 + values8\n",
    "values01 = values01 + values1\n",
    "values06 = values06 + values6\n",
    "print(\"Valor final de los conjuntos de valores 08,01 y 06:\")\n",
    "print(len(values08))\n",
    "print(len(values01))\n",
    "print(len(values06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy.amin(values08) )\n",
    "print(numpy.amax(values08) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutaArchivosClima = path.dirname(path.abspath(__file__)) +'\\datos\\datosAemetJuntos'\n",
    "#rutaArchivosClima = 'datos/datosAemetJuntos'\n",
    "#rutaArchivosClima = 'datos/datosAemet141516'\n",
    "rutaArchivosClima = 'datos/datosAemet14151617Abril18'\n",
    "cabecerasClima = ['Estación','Provincia','Temperatura máxima (ºC)','Temperatura mínima (ºC)','Temperatura media (ºC)','Racha (km/h)','Velocidad máxima (km/h)','Precipitación 00-24h (mm)','Precipitación 00-06h (mm)','Precipitación 06-12h (mm)','Precipitación 12-18h (mm)','Precipitación 18-24h (mm)']\n",
    "#dfClima = cargaDfClima(rutaArchivosClima,cabecerasClima,'Madrid, Retiro')\n",
    "dfClima = cargaDfClima(rutaArchivosClima,cabecerasClima,'Madrid, Ciudad Universitaria')\n",
    "#print(dfClima.head())\n",
    "print(dfClima.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ---------- Construimos el dataset con los valores que queremos predecir ---------- \"\"\"\n",
    "valuesFinal= numpy.zeros((1,dfClima.shape[1]+1)) ##valuesFinal= numpy.zeros((1,13))\n",
    "#dfClima = dfClima.fillna(0) #rellena con 0 los nulos - Cambiar a que coja lo del día de antes o después\n",
    "#controlar que no haya nulos el primer dia\n",
    "#Para poner el valor medio: df['A'].fillna(df['A'].median())\n",
    "#Para limitarlo a que se haga en los primeros: df = df.fillna(value='missing', method='bfill', limit=1)\n",
    "dfClima = dfClima.fillna(method='ffill') #rellena los nulos con el valor del día de antes\n",
    "matrizClima = dfClima.reset_index().values\n",
    "print(matrizClima.shape)\n",
    "print(len(values08))\n",
    "for i in range (0,len(values08),1):  \n",
    "    newValueArr=numpy.array([[values08[i], values01[i], values06[i], quitarHora(matrizClima[i][3]), quitarHora(matrizClima[i][4]), matrizClima[i][5], quitarHora(matrizClima[i][6]), quitarHora(matrizClima[i][7]), matrizClima[i][8], matrizClima[i][9], matrizClima[i][10], matrizClima[i][11], matrizClima[i][12]]]).astype(float)     \n",
    "    valuesFinal=numpy.append(valuesFinal, newValueArr, axis = 0)\n",
    "print(valuesFinal[0])\n",
    "valuesFinal = numpy.delete(valuesFinal, 0, axis=0) #Eliminamos el primero que inicializamos\n",
    "print(valuesFinal[0])\n",
    "print(len(valuesFinal))\n",
    "#print(valuesFinal[730])\n",
    "#print(valuesFinal[731])\n",
    "print('===...===')\n",
    "numpy.nan_to_num(valuesFinal,copy=False)\n",
    "print(valuesFinal[0])\n",
    "print('======')\n",
    "#print(values08[730])\n",
    "#print(values08[731])\n",
    "print(values08[0])\n",
    "#print('::::::::::::')\n",
    "#print(valuesFinal[1000])\n",
    "#print(valuesFinal[1095])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -------------------- normalize features -------------------- \"\"\"\n",
    "#Hay que normalizar la matriz resultante para ello usamos MinMaxScaler() y scaler()\n",
    "#scaler (son los valores normalizados) necesita el formato => Filas = Dias, Columnas = ValorCadaCosaQueMide\n",
    "scaler_VF = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaled = scaler.fit_transform(valuesFinal)\n",
    "scaler_fit = scaler_VF.fit(valuesFinal)\n",
    "scaler_tra = scaler_VF.transform(valuesFinal)\n",
    "scaled = scaler_tra\n",
    "print(scaled.shape) #sera 366 + 365\n",
    "print(scaled[0])\n",
    "print('------')\n",
    "print(scaled[1])\n",
    "print('------')\n",
    "print(scaled[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" preparamos los dos conjuntos (datosDiasPrevios,DiaSiguiente)\"\"\"\n",
    "#n_chars = len(raw_text)\n",
    "#nvocab = len(chars)\n",
    "n_examples = len(valuesFinal)\n",
    "# prepare the dataset of input to output pairs encoded as integers \n",
    "seq_length = 15 #coger sólo 15 días\n",
    "dataX = [] \n",
    "dataY = [] \n",
    "for i in range(0, n_examples - seq_length, 1): \n",
    "    seq_in = scaled[i:i + seq_length] \n",
    "    seq_out = scaled[i + seq_length] \n",
    "    dataX.append(seq_in) \n",
    "    dataY.append(seq_out) \n",
    "n_patterns = len(dataX) \n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "print(len(dataX))\n",
    "print(len(dataY))\n",
    "#for j in range(0, 3):\n",
    "#    print('====')\n",
    "#    print(dataX[j])\n",
    "#    print(':::::')\n",
    "#    print(dataY[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ::::::::::::::::::::  :::::::::::::::::::: \"\"\"\n",
    "#Train el 80% y Test el 20%\n",
    "#Ahora hay 365 filas *2 y 26 columnas\n",
    "#values = reframed.values\n",
    "#n_train_days = round((values.shape[0]+1)*0.8)#292 #80% de 365\n",
    "#n_train_days = round(730)#292 #80% de 365 si queremos 2 años de entrenamiento 730\n",
    "n_train_days = round(1446)#292 #80% de 365 si queremos 2 años de entrenamiento 730\n",
    "train_X=numpy.array(dataX[:n_train_days])\n",
    "train_y=numpy.array(dataY[:n_train_days])\n",
    "test_X=numpy.array(dataX[n_train_days:])\n",
    "test_y=numpy.array(dataY[n_train_days:])\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "#train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "#test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "print('===')\n",
    "#print(train_X[364])\n",
    "#print(train_y[0])\n",
    "#print(values[365][17\n",
    "print(train_X.shape[1])\n",
    "print(train_X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numpy.savetxt('test_X.csv', test_X, fmt='%.2f', delimiter=',')\n",
    "#numpy.savetxt('test_X_No2.csv', values08, fmt='%.2f', delimiter=',')\n",
    "#print(test_X.shape)\n",
    "#te=[]\n",
    "#print(test_X[0])\n",
    "#print(test_X[0].shape)\n",
    "#print(test_X.shape)\n",
    "#print(test_y.shape)\n",
    "#print(numpy.array(dataX[n_train_days]))\n",
    "from keras.models import load_model\n",
    "#model = load_model('my_model_2018-04-21_18-16-14.h5')\n",
    "#loaded_model = load_model('checkpoint/weights2018-04-21_18-16-14.hdf5')\n",
    "#desde que contamos con 14,15,16,17 para entrenar y 18 para test\n",
    "#loaded_model = load_model('checkpoint/weights2018-05-22_02-39-19.hdf5')\n",
    "#loaded_model = load_model('checkpoint/weights2018-05-23_09-19-56.hdf5\n",
    "\n",
    "loaded_model = load_model('checkpoint/weights2018-06-02_21-19-29.hdf5')\n",
    "\n",
    "# evaluate loaded model on test data \n",
    "# Define X_test & Y_test data first\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(test_X, test_y, verbose=0)\n",
    "#print (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_obtenido = []\n",
    "lista_real = []\n",
    "dias = 100\n",
    "primer_dia = 0\n",
    "variable_predicha = 0\n",
    "error = 0\n",
    "\n",
    "\n",
    "for i in range(primer_dia,primer_dia+dias):\n",
    "    newPred = loaded_model.predict(train_X[i].reshape(1,15,13), verbose=0)\n",
    "    real = train_y[i,variable_predicha]\n",
    "    predicho = newPred[0,variable_predicha]\n",
    "    error += abs(real-predicho)\n",
    "    lista_real.append(real)\n",
    "    lista_obtenido.append(predicho)\n",
    "\n",
    "print(\"El error medio es: \", error / dias)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "grp = plt.figure(figsize=(20,5))\n",
    "plt.plot(lista_real,\"o-\",color=\"blue\",label=\"real\")\n",
    "plt.plot(lista_obtenido,\"+-\",color=\"red\",label=\"estimado\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[3,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista_obtenido = []\n",
    "lista_real = []\n",
    "dias = 100\n",
    "primer_dia = 0\n",
    "variable_predicha = 0\n",
    "error = 0\n",
    "desvTest = []\n",
    "\n",
    "for i in range(primer_dia,primer_dia+dias):\n",
    "    newPred = loaded_model.predict(test_X[i].reshape(1,15,13), verbose=0)\n",
    "    real = test_y[i,variable_predicha]\n",
    "    predicho = newPred[0,variable_predicha]\n",
    "    error += abs(real-predicho)\n",
    "    #print(\"El valor real es: \", real,' y el predicho es: ', predicho)\n",
    "    desvTest.append(abs(real-predicho))\n",
    "    lista_real.append(real)\n",
    "    lista_obtenido.append(predicho)\n",
    "\n",
    "print(\"El error medio es: \", error / dias)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "grp = plt.figure(figsize=(20,5))\n",
    "plt.plot(lista_real,\"o-\",color=\"blue\",label=\"real\")\n",
    "plt.plot(lista_obtenido,\"+-\",color=\"red\",label=\"estimado\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.set_printoptions(suppress=True)\n",
    "newPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newPred = loaded_model.predict(test_X, verbose=1)\n",
    "\n",
    "#desde = 1\n",
    "#for d in range(0,351):\n",
    "#    te.append(test_X[d][14][0])\n",
    "#print(test_X[2][14][0])\n",
    "#print(\"==\")\n",
    "#numpy.savetxt('test_X.csv', te, fmt='%.2f', delimiter=',')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "a=[]\n",
    "b=[]\n",
    "\n",
    "desde = 100\n",
    "dias = 20\n",
    "for dia in range(desde,desde+dias):\n",
    "    a.append(test_X[dia-14][14][0])\n",
    "    b.append(newPred[dia-15][0])\n",
    "    #print(dia,test_X[dia-14][14][0],yhat[dia-15][0])\n",
    "\n",
    "grp = plt.figure(figsize=(20,5))\n",
    "plt.plot(a,\"o-\",color=\"blue\",label=\"real\")\n",
    "plt.plot(b,\"+-\",color=\"red\",label=\"estimado\")\n",
    "plt.ylabel(\"cantidad\")\n",
    "plt.title(\"NO2 en Plaza España (normalizado)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular la desvición típica\n",
    "print('Desviación típica: ',numpy.std(desvTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meteo]",
   "language": "python",
   "name": "conda-env-meteo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
