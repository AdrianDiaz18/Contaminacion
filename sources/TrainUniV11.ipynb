{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.lbGraficas import cargaDataSet\n",
    "from lib.lbGraficas import showGraficaCont\n",
    "from lib.lbClima import cargaDfClima\n",
    "from lib.lbClima import quitarHora\n",
    "from lib.lbVarios import series_to_supervised\n",
    "from lib.lbCargaDatosNorm import cargaDatosSep\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from os import path\n",
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ya no hace falta esta carga\"\"\"\n",
    "#dfSol = cargaDatosSep('datos/datosContaminacion/datos17.csv',';')\n",
    "#print (\"=======\")\n",
    "#print (dfSol)\n",
    "#print (dfSol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "hInicio = time.strftime(\"%H-%M-%S\")\n",
    "#print (time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "# load the network weights\n",
    "#filepath=\"checkpoint/weightsV1.hdf5\"\n",
    "#model.load_weights(filepath) #Descomentar cuando tengamos un checkpoint\n",
    "#fecha_archivos = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#fecha_archivos = time.strftime(\"%Y-%m-%d_%H)\n",
    "#print(fecha_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -------------------- load dataset -------------------- \"\"\"\n",
    "dataset14 = cargaDataSet(\"datos/datosContaminacion/datos2014_DatosDiarios.txt\") #datos 2014\n",
    "dataset15 = cargaDataSet(\"datos/datosContaminacion/datos2015_DatosDiarios.txt\") #datos 2015\n",
    "dataset16 = cargaDataSet(\"datos/datosContaminacion/datos2016_DatosDiarios.txt\") #datos 2016\n",
    "#dataset = pandas.concat([dataset15,dataset16])\n",
    "#dataset = pandas.concat([dataset14,dataset15,dataset16])\n",
    "#print(dataset.head())\n",
    "print(dataset15.shape)\n",
    "print(dataset16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para 2017 y 2018 cambia el formato del txt, entonces lo cogemos en csv ya directamente\n",
    "dataset17 = pandas.read_csv('datos/datosContaminacion/datos17.csv',sep=';')\n",
    "df17 = dataset17.drop(['V01', 'V02', 'V03', 'V04', 'V05', 'V06', 'V07', 'V08', 'V09', 'V10','V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20','V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31'], axis=1)\n",
    "df17['Tecnica']='00'\n",
    "df17['Periodo_Analisis']='04'\n",
    "df17Filtrado = df17.loc[df17['ESTACION'] == 4]\n",
    "df17Filtrado['ESTACION']='28079004'\n",
    "dataset17 = df17Filtrado[['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']]\n",
    "#dataset17 = df17Filtrado.loc['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']\n",
    "dataset17.columns = ['Estacion','Magnitud','Tecnica','Periodo_Analisis','Año','Mes',\"Dia1\",\"Dia2\",\"Dia3\",\"Dia4\",\"Dia5\",\"Dia6\",\"Dia7\",\"Dia8\",\"Dia9\",\"Dia10\",\"Dia11\",\"Dia12\",\"Dia13\",\"Dia14\",\"Dia15\",\"Dia16\",\"Dia17\",\"Dia18\",\"Dia19\",\"Dia20\",\"Dia21\",\"Dia22\",\"Dia23\",\"Dia24\",\"Dia25\",\"Dia26\",\"Dia27\",\"Dia28\",\"Dia29\",\"Dia30\",\"Dia31\"]\n",
    "print(dataset17.shape)\n",
    "\n",
    "dataset18 = pandas.read_csv('datos/datosContaminacion/datos18.csv',sep=';')\n",
    "df18 = dataset18.drop(['V01', 'V02', 'V03', 'V04', 'V05', 'V06', 'V07', 'V08', 'V09', 'V10','V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20','V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31'], axis=1)\n",
    "df18['Tecnica']='00'\n",
    "df18['Periodo_Analisis']='04'\n",
    "df18Filtrado = df18.loc[df18['ESTACION'] == 4]\n",
    "df18Filtrado['ESTACION']='28079004'\n",
    "dataset18 = df18Filtrado[['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']]\n",
    "#dataset18 = df18Filtrado.loc['ESTACION','MAGNITUD','Tecnica','Periodo_Analisis','ANO','MES','D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']\n",
    "dataset18.columns = ['Estacion','Magnitud','Tecnica','Periodo_Analisis','Año','Mes',\"Dia1\",\"Dia2\",\"Dia3\",\"Dia4\",\"Dia5\",\"Dia6\",\"Dia7\",\"Dia8\",\"Dia9\",\"Dia10\",\"Dia11\",\"Dia12\",\"Dia13\",\"Dia14\",\"Dia15\",\"Dia16\",\"Dia17\",\"Dia18\",\"Dia19\",\"Dia20\",\"Dia21\",\"Dia22\",\"Dia23\",\"Dia24\",\"Dia25\",\"Dia26\",\"Dia27\",\"Dia28\",\"Dia29\",\"Dia30\",\"Dia31\"]\n",
    "print(dataset18.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.concat([dataset14,dataset15,dataset16,dataset17,dataset18])\n",
    "print(dataset.shape)\n",
    "plzEspana = \"28079004\"\n",
    "print(\"ahora filtramos y colocamos todos los días en orden\")\n",
    "values08 = showGraficaCont(dataset,plzEspana,\"08\",False) #valor de NO2 en cada día\n",
    "values8 = showGraficaCont(dataset,plzEspana,\"8\",True) #valor de NO2 en cada día\n",
    "values01 = showGraficaCont(dataset,plzEspana,\"01\",False) #valor de SO2 en cada día\n",
    "values1 = showGraficaCont(dataset,plzEspana,\"1\",True) #valor de SO2 en cada día\n",
    "values06 = showGraficaCont(dataset,plzEspana,\"06\",False) #valor de CO en cada día\n",
    "values6 = showGraficaCont(dataset,plzEspana,\"6\",True) #valor de CO en cada día\n",
    "print(\"Número de valores de 08: \" + str(len(values08)) + \" Y número de valores de 8: \" + str(len(values8)))\n",
    "print(\"Número de valores de 01: \" + str(len(values01)) + \" Y número de valores de 1: \" + str(len(values1)))\n",
    "print(\"Número de valores de 06: \" + str(len(values06)) + \" Y número de valores de 6: \" + str(len(values6)))\n",
    "values08 = values08 + values8\n",
    "values01 = values01 + values1\n",
    "values06 = values06 + values6\n",
    "print(\"Valor final de los conjuntos de valores 08,01 y 06:\")\n",
    "print(len(values08))\n",
    "print(len(values01))\n",
    "print(len(values06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutaArchivosClima = path.dirname(path.abspath(__file__)) +'\\datos\\datosAemetJuntos'\n",
    "#rutaArchivosClima = 'datos/datosAemetJuntos'\n",
    "#rutaArchivosClima = 'datos/datosAemet141516'\n",
    "rutaArchivosClima = 'datos/datosAemet14151617Abril18'\n",
    "cabecerasClima = ['Estación','Provincia','Temperatura máxima (ºC)','Temperatura mínima (ºC)','Temperatura media (ºC)','Racha (km/h)','Velocidad máxima (km/h)','Precipitación 00-24h (mm)','Precipitación 00-06h (mm)','Precipitación 06-12h (mm)','Precipitación 12-18h (mm)','Precipitación 18-24h (mm)']\n",
    "#dfClima = cargaDfClima(rutaArchivosClima,cabecerasClima,'Madrid, Retiro')\n",
    "dfClima = cargaDfClima(rutaArchivosClima,cabecerasClima,'Madrid, Ciudad Universitaria')\n",
    "#print(dfClima.head())\n",
    "print(dfClima.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ---------- Construimos el dataset con los valores que queremos predecir ---------- \"\"\"\n",
    "valuesFinal= numpy.zeros((1,dfClima.shape[1]+1)) ##valuesFinal= numpy.zeros((1,13))\n",
    "#dfClima = dfClima.fillna(0) #rellena con 0 los nulos - Cambiar a que coja lo del día de antes o después\n",
    "#controlar que no haya nulos el primer dia\n",
    "#Para poner el valor medio: df['A'].fillna(df['A'].median())\n",
    "#Para limitarlo a que se haga en los primeros: df = df.fillna(value='missing', method='bfill', limit=1)\n",
    "dfClima = dfClima.fillna(method='ffill') #rellena los nulos con el valor del día de antes\n",
    "matrizClima = dfClima.reset_index().values\n",
    "print(matrizClima.shape)\n",
    "print(len(values08))\n",
    "for i in range (0,len(values08),1):  \n",
    "    newValueArr=numpy.array([[values08[i], values01[i], values06[i], quitarHora(matrizClima[i][3]), quitarHora(matrizClima[i][4]), matrizClima[i][5], quitarHora(matrizClima[i][6]), quitarHora(matrizClima[i][7]), matrizClima[i][8], matrizClima[i][9], matrizClima[i][10], matrizClima[i][11], matrizClima[i][12]]]).astype(float)     \n",
    "    valuesFinal=numpy.append(valuesFinal, newValueArr, axis = 0)\n",
    "print(valuesFinal[0])\n",
    "valuesFinal = numpy.delete(valuesFinal, 0, axis=0) #Eliminamos el primero que inicializamos\n",
    "print(valuesFinal[0])\n",
    "print(len(valuesFinal))\n",
    "#print(valuesFinal[730])\n",
    "#print(valuesFinal[731])\n",
    "print('===...===')\n",
    "numpy.nan_to_num(valuesFinal,copy=False)\n",
    "print(valuesFinal[0])\n",
    "print('======')\n",
    "#print(values08[730])\n",
    "#print(values08[731])\n",
    "print(values08[0])\n",
    "#print('::::::::::::')\n",
    "#print(valuesFinal[1000])\n",
    "#print(valuesFinal[1095])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -------------------- normalize features -------------------- \"\"\"\n",
    "#Hay que normalizar la matriz resultante para ello usamos MinMaxScaler() y scaler()\n",
    "#scaler (son los valores normalizados) necesita el formato => Filas = Dias, Columnas = ValorCadaCosaQueMide\n",
    "scaler_VF = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaled = scaler.fit_transform(valuesFinal)\n",
    "scaler_fit = scaler_VF.fit(valuesFinal)\n",
    "scaler_tra = scaler_VF.transform(valuesFinal)\n",
    "scaled = scaler_tra\n",
    "print(scaled.shape) #sera 366 + 365\n",
    "print(scaled[0])\n",
    "print('------')\n",
    "print(scaled[1])\n",
    "print('------')\n",
    "print(scaled[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" preparamos los dos conjuntos (datosDiasPrevios,DiaSiguiente)\"\"\"\n",
    "#n_chars = len(raw_text)\n",
    "#nvocab = len(chars)\n",
    "n_examples = len(valuesFinal)\n",
    "# prepare the dataset of input to output pairs encoded as integers \n",
    "seq_length = 15 #coger sólo 15 días\n",
    "dataX = [] \n",
    "dataY = [] \n",
    "for i in range(0, n_examples - seq_length, 1): \n",
    "    seq_in = scaled[i:i + seq_length] \n",
    "    seq_out = scaled[i + seq_length] \n",
    "    dataX.append(seq_in) \n",
    "    dataY.append(seq_out) \n",
    "n_patterns = len(dataX) \n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "print(len(dataX))\n",
    "print(len(dataY))\n",
    "#for j in range(0, 3):\n",
    "#    print('====')\n",
    "#    print(dataX[j])\n",
    "#    print(':::::')\n",
    "#    print(dataY[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataX[0].shape)\n",
    "print(len(dataX))\n",
    "print(dataY[0].shape)\n",
    "print(len(dataY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" :::::::::::::::::::: Define and Fit Model :::::::::::::::::::: \"\"\"\n",
    "#Train el 80% y Test el 20%\n",
    "#Ahora hay 365 filas *2 y 26 columnas\n",
    "#values = reframed.values\n",
    "#n_train_days = round((values.shape[0]+1)*0.8)#292 #80% de 365\n",
    "#n_train_days = round(730)#292 #80% de 365 si queremos 2 años de entrenamiento 730\n",
    "n_train_days = round(1446)#292 #80% de 365 si queremos 2 años de entrenamiento 730\n",
    "train_X=numpy.array(dataX[:n_train_days])\n",
    "train_y=numpy.array(dataY[:n_train_days])\n",
    "test_X=numpy.array(dataX[n_train_days:])\n",
    "test_y=numpy.array(dataY[n_train_days:])\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "#train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "#test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "print('===')\n",
    "#print(train_X[364])\n",
    "#print(train_y[0])\n",
    "#print(values[365][17\n",
    "print(train_X.shape[1])\n",
    "print(train_X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.savetxt('test_X.csv', test_X, fmt='%.2f', delimiter=',')\n",
    "#numpy.savetxt('test_X_No2.csv', values08, fmt='%.2f', delimiter=',')\n",
    "#print(test_X.shape)\n",
    "#te=[]\n",
    "#print(test_X[0])\n",
    "print(test_X[0].shape)\n",
    "#desde = 1\n",
    "#for d in range(0,351):\n",
    "#    te.append(test_X[d][14][0])\n",
    "#print(test_X[2][14][0])\n",
    "#print(\"==\")\n",
    "#numpy.savetxt('test_X.csv', te, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" design network\"\"\"\n",
    "oper = 100\n",
    "numCapas = 1\n",
    "model = Sequential()\n",
    "#shape va  aser (numeroEjemplosTrain,numroDeDiasAtenerEnCuenta,NumeroDeVariablesEnCadaDia)\n",
    "#model.add(LSTM(1000, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "#input_shape= (numCasosAnteriores,Columnas)\n",
    "#model.add(LSTM(500, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "# antes estaba esta model.add(LSTM(oper, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "#model.add(LSTM(oper, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "#Para 1 capa: model.add(LSTM(oper, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(oper, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "#model.add(LSTM(1000, return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(oper, return_sequences=True))\n",
    "#for capa in range(1,numCapas-1):\n",
    "#    model.add(LSTM(oper, return_sequences=True))\n",
    "#    print(capa+1)\n",
    "\n",
    "    #model.add(LSTM(oper, return_sequences=True))\n",
    "#model.add(LSTM(oper, return_sequences=True))\n",
    "#con esta model.add(LSTM(oper))\n",
    "#model.add(LSTM(oper))\n",
    "print('Total de capas: ' + str((numCapas)))\n",
    "model.add(Dropout(0.8)) #Previene el sobreentrenamiento(overfitting) pero no sé más.\n",
    "model.add(Dense(13, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#probar con loss=categorical_crossentropy  // opt = SGD(lr=0.1, momentum=0.3) o puede ser poniendo 'sgd' directamente\n",
    "#para opt=RMSprop podemos probar tambien\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "#import time\n",
    "#import datetime\n",
    "#print (time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "# load the network weights\n",
    "#filepath=\"checkpoint/weightsV1.hdf5\"\n",
    "#model.load_weights(filepath) #Descomentar cuando tengamos un checkpoint\n",
    "fecha_archivos = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#fecha_archivos = time.strftime(\"%Y-%m-%d_%H\")\n",
    "print(fecha_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "# comentar una vez guardado\n",
    "filepath=\"checkpoint/weights\" + fecha_archivos + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./log', histogram_freq=0, batch_size=8, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "callbacks_list = [checkpoint,tb]\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "test_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Si ya esta entrenada y cargamos unos pesos, no hace falta entrenrla de nuevo\n",
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#history = model.fit(train_X, train_y, epochs=20000, batch_size=len(train_X))\n",
    "#history = model.fit(train_X, train_y, epochs=30000, batch_size=len(train_X), callbacks=callbacks_list) #con el checkpoint\n",
    "#history = model.fit(train_X, train_y, epochs=80000, batch_size=32) #con el checkpoint\n",
    "#history = model.fit(train_X, train_y, epochs=300, batch_size=32,callbacks=callbacks_list, verbose=1) #con el checkpoint\n",
    "ep = 62\n",
    "bat_sze = 32 #Al tener en cuenta los ultimos 15 dias, ponemos un batch de 16 o podemos probar con 8, siendo siempre un dividendo del total de ejemplos\n",
    "history = model.fit(train_X, train_y, epochs=ep, batch_size=bat_sze,callbacks=callbacks_list, validation_data=(test_X,test_y), verbose=1) #con el checkpoint\n",
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss\n",
    "#si ya esta entrenada, no hace falta evaluar\n",
    "#Podemos evaluar con datos que no sean los de entrenamiento (quizas sea mejor para saber como funciona, ¿no?)\n",
    "#evaluar con test_X y test_Y\n",
    "loss, acc = model.evaluate(test_X, test_y, verbose=1)\n",
    "print('======')\n",
    "print(loss)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('my_model_' + fecha_archivos + '.h5')\n",
    "file = open('log/loss.txt','a') \n",
    "fin_ejec = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file.write('' + fecha_archivos + ' => op: ' + str(oper) + ', ' + str(numCapas) + ' capas, epochs: ' + str(ep) + ', batch_size: ' + str(bat_sze) + ', loss(evaluate con test): ' + str(loss) + ', acc(evaluate con test): ' + str(acc) + ', HoraInicio: ' + hInicio + ', HoraFinal: ' + fin_ejec + '\\n')\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "yhat = model.predict(test_X, verbose=1)\n",
    "file = open('predict/predict_' + fecha_archivos + '.txt','a') \n",
    "for x in yhat:\n",
    "    file.write(str(x)+'\\n')\n",
    "file.close() \n",
    "#zzz=scaler.fit_transform(yhat)\n",
    "#print(zzz)\n",
    "#leyendo un poco, no entiendo esto: For a multiclass classification problem, the results may be in the form of an array of probabilities (assuming a one hot encoded output variable) that may need to be converted to a single class output prediction using the argmax() NumPy function.\n",
    "#https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "\n",
    "desde = 1\n",
    "dias = 30\n",
    "for dia in range(desde,desde+dias):\n",
    "    a.append(test_X[dia-14][14][0])\n",
    "    b.append(yhat[dia-15][0])\n",
    "    #print(dia,test_X[dia-14][14][0],yhat[dia-15][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grp = plt.figure(figsize=(20,5))\n",
    "plt.plot(a,\"o-\",color=\"blue\",label=\"real\")\n",
    "plt.plot(b,\"+-\",color=\"red\",label=\"estimado\")\n",
    "plt.ylabel(\"cantidad\")\n",
    "plt.title(\"NO2 en Plaza España (normalizado) desde el dia: \" + str(desde) + \" Hasta: \" + str((desde+dias)) + \" => op: \" + str(oper) + \", epochs: \" + str(ep) + \", batch_size: \" + str(bat_sze) + \" Capas: \" + str(numCapas) + \" \")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#grp.savefig('predict/grpPredictEne2017No2PlzEsp_' + fecha_archivos + '.png')\n",
    "grp.savefig('predict/grpPredictEne2017No2PlzEsp_' + fecha_archivos + '.png')\n",
    "#fig.savefig('temp.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "\n",
    "desde = 16\n",
    "dias = 46\n",
    "for dia in range(desde,desde+dias):\n",
    "    a.append(test_X[dia-14][14][0])\n",
    "    b.append(yhat[dia-15][0])\n",
    "    #print(dia,test_X[dia-14][14][0],yhat[dia-15][0])\n",
    "\n",
    "grp = plt.figure(figsize=(20,5))\n",
    "plt.plot(a,\"o-\",color=\"blue\",label=\"real\")\n",
    "plt.plot(b,\"+-\",color=\"red\",label=\"estimado\")\n",
    "plt.ylabel(\"cantidad\")\n",
    "plt.title(\"NO2 en Plaza España (normalizado) desde el dia: \" + str(desde) + \" Hasta: \" + str((desde+dias)) + \" => op: \" + str(oper) + \", epochs: \" + str(ep) + \", batch_size: \" + str(bat_sze) + \" Capas: \" + str(numCapas) + \" \")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#grp.savefig('predict/grpPredictEne2017No2PlzEsp_' + fecha_archivos + '.png')\n",
    "grp.savefig('predict/grpPredictEne2017No2PlzEsp_' + fecha_archivos + 'de16a46.png')\n",
    "#fig.savefig('temp.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.savetxt('predict/predict_' + fecha_archivos + '_Normaliz.csv', yhat, fmt='%.2f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\"\"\" -------------------- COMPARAR GRAFICAS solucion -------------------- \"\"\"\n",
    "#dfSol = pandas.read_csv('datos/datosContaminacion/datos17.csv',sep=';')\n",
    "##NO2 es magnitud=8 & Pza. de España = punto muestreo: 28079004\n",
    "##df.loc[(df['column_name'] == some_value) & df['other_column'].isin(some_values)]\n",
    "#dfNO2=dfSol.loc[df['MAGNITUD'] == 8]\n",
    "#dfPlzEsp = dfSol[dfSol['PUNTO_MUESTREO'].str.contains(\"28079004\")]\n",
    "##df.loc[df['PUNTO_MUESTREO'] == '28079004']\n",
    "#dfEneNo2PlzEsp = dfSol.loc[ (dfSol['MAGNITUD'] == 8)  &  (dfSol['PUNTO_MUESTREO'].str.contains(\"28079004\")) & (dfSol['MES'] == 1) ]\n",
    "#dfEneNo2PlzEsp\n",
    "#npDfEneNo2PlzEsp = dfEneNo2PlzEsp.as_matrix(['D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'])\n",
    "##plt.xlabel(\"día del mes\")\n",
    "##plt.ylabel(\"cantidad\")\n",
    "##plt.title(\"NO2 en Plaza España Enero 2018\")\n",
    "##plt.plot(npDfEneNo2PlzEsp[0])\n",
    "##plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\" -------------------- normalize features -------------------- \"\"\"\n",
    "#dfEneNo2PlzEspFin = dfEneNo2PlzEsp[ ['D01','D02','D03','D04','D05','D06','D07','D08','D09','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'] ]\n",
    "#arr_No2Ene17 = [npDfEneNo2PlzEsp[0]]\n",
    "#valuesFinalSol= numpy.zeros((1,1))\n",
    "#for cant in arr_No2Ene17[0]:\n",
    "#    valuesFinal=numpy.append(valuesFinalSol, [[cant]], axis = 0)\n",
    "#valuesFinal = numpy.delete(valuesFinalSol, 0, axis=0)\n",
    "#scaler_VFSol = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaler_fitSol = scaler_VF.fit(valuesFinalSol)\n",
    "#scaler_traSol = scaler_VF.transform(valuesFinalSol)\n",
    "#scaledSol = scaler_traSol\n",
    "#print(scaledSol.shape) #sera 366 + 365\n",
    "#print(scaledSol) #sera 366 + 365\n",
    "#plt.xlabel(\"día del mes\")\n",
    "#plt.ylabel(\"cantidad\")\n",
    "#plt.title(\"NO2 en Plaza España Enero 2018(normalizado)\")\n",
    "#plt.plot(lf,fdSol)\n",
    "##plt.plot(lf,dfSol)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meteo]",
   "language": "python",
   "name": "conda-env-meteo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
